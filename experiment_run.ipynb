{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wacgen' from 'wacgen.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wacgen\n",
    "reload(wacgen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prepare test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "load and merge csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testpath = './SAIA_Data/test_set_ground_truth'\n",
    "atest = pd.read_csv(testpath+'/ground_truth_a.csv')\n",
    "btest = pd.read_csv(testpath+'/ground_truth_b.csv')\n",
    "ctest = pd.read_csv(testpath+'/ground_truth_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "atest['Testset'] = 'A'\n",
    "btest['Testset'] = 'B'\n",
    "ctest['Testset'] = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltest = atest.append(btest)\n",
    "alltest = alltest.append(ctest)\n",
    "len(alltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltest['prev_file'] = alltest['Image Number'].shift()\n",
    "alltest['same'] = alltest['Image Number'] == alltest['prev_file']\n",
    "alltest = alltest[alltest['same'] == False]\n",
    "len(alltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abs. Location</th>\n",
       "      <th>Category</th>\n",
       "      <th>Color</th>\n",
       "      <th>Entry-Level</th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Image Number</th>\n",
       "      <th>Other</th>\n",
       "      <th>Referring Expression</th>\n",
       "      <th>Rel. Location</th>\n",
       "      <th>Rel. Object</th>\n",
       "      <th>Size</th>\n",
       "      <th>Testset</th>\n",
       "      <th>prev_file</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hat</td>\n",
       "      <td>5383913d793954ad2d000035</td>\n",
       "      <td>1001_4.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>parrot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bird</td>\n",
       "      <td>537fb928d31907d16d000cac</td>\n",
       "      <td>10028_1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbird</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1001_4.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mountain</td>\n",
       "      <td>537fb7b5d31907d16d000b04</td>\n",
       "      <td>10065_4.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mountain skyline outline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>10028_1.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cake</td>\n",
       "      <td>537fc095d31907d16d001577</td>\n",
       "      <td>10143_1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>10065_4.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>painting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>painting</td>\n",
       "      <td>53839350793954ad2d0000ea</td>\n",
       "      <td>10172_3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>big painting on left of wall</td>\n",
       "      <td>prep_on_left</td>\n",
       "      <td>wall</td>\n",
       "      <td>big</td>\n",
       "      <td>A</td>\n",
       "      <td>10143_1.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abs. Location  Category Color Entry-Level                Identifier  \\\n",
       "0            NaN       hat   NaN         hat  5383913d793954ad2d000035   \n",
       "3            NaN    parrot   NaN        bird  537fb928d31907d16d000cac   \n",
       "6            NaN      hill   NaN    mountain  537fb7b5d31907d16d000b04   \n",
       "9            NaN      dish   NaN        cake  537fc095d31907d16d001577   \n",
       "12           NaN  painting   NaN    painting  53839350793954ad2d0000ea   \n",
       "\n",
       "   Image Number Other          Referring Expression Rel. Location Rel. Object  \\\n",
       "0    1001_4.jpg   NaN                           Hat           NaN         NaN   \n",
       "3   10028_1.jpg   NaN                         bbird           NaN         NaN   \n",
       "6   10065_4.jpg   NaN      mountain skyline outline           NaN         NaN   \n",
       "9   10143_1.jpg   NaN                          cake           NaN         NaN   \n",
       "12  10172_3.jpg   NaN  big painting on left of wall  prep_on_left        wall   \n",
       "\n",
       "   Size Testset    prev_file   same  \n",
       "0   NaN       A          NaN  False  \n",
       "3   NaN       A   1001_4.jpg  False  \n",
       "6   NaN       A  10028_1.jpg  False  \n",
       "9   NaN       A  10065_4.jpg  False  \n",
       "12  big       A  10143_1.jpg  False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "files = [int(i.split('_')[0]) for i in alltest['Image Number']]\n",
    "regions = [int(i.split('_')[1][:-4]) for i in alltest['Image Number']]\n",
    "\n",
    "alltest['file'] = files\n",
    "alltest['region'] = regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Experiment 1: Non-interactive REG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this experiment, we generate referring expressions with different wac (words-as-classifiers) models.\n",
    "This model associates words with visual classifiers, and can be trained on different sets of visual features.\n",
    "We will compare a basic set of visual features provided with the SAIA image corpus (features.mat) and features extracted with a ConvNet (googLE).\n",
    "\n",
    "As this is a non-interactive experiment, we generate a single expression for each object in our test set, i.e. the optimal expression predicted by the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Apply generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we now generate refexp. for the RefeitGame\n",
    "* if you want, you can skip this step and use the file\n",
    "*EvalExp1/exp1_ground_truth_generated.csv*\n",
    "* the following steps will show how to produce this file for the given generation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "load model trained on visual features provided with the saia corpus\n",
    "(very basic colour and spatial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers ['* Started 2015-12-03_19:25\\n', \"Namespace(classf='logreg', feat_path='InData/features.mat', filter_rel=True, googLe=False, googSaia=False, just_splits=False, n_folds=10, n_neg=7, neg_from='random', out_path='OutData/Models/groundsplit_400saiafr7/', rex_path='InData/RealGames.txt', split_path='OutData/Splits/ground-truth-split.pklz', wrdl_crit=['most_common', '400'])\"]\n"
     ]
    }
   ],
   "source": [
    "gensaia = wacgen.WacGen(saia=True,google=False,groundsplit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "in addition to wac, we need a way to decide how many words the expression for a given object should contain\n",
    "we do this in a very simple way and train some classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len 1\n",
      "Train len 2\n",
      "Train len 3\n",
      "Train len 4\n",
      "Train len 5\n"
     ]
    }
   ],
   "source": [
    "_,_,lensaia = gensaia.get_len_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saia_gen_list = []\n",
    "for (ix,row) in alltest.iterrows():\n",
    "    #print (row['file'],row['region'])\n",
    "    gen_utt2 = gensaia.generate_beam_plen(lensaia,row['file'],row['region'])\n",
    "    #print gen_utt2\n",
    "    saia_gen_list.append(' '.join(gen_utt2[1:]))\n",
    "    \n",
    "alltest['saia'] = saia_gen_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now load the model based on visual features extracted with a ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers ['* Started 2015-12-03_19:24\\n', \"Namespace(classf='logreg', feat_path='InData/features.mat', filter_rel=True, googLe=True, googSaia=False, just_splits=False, n_folds=10, n_neg=7, neg_from='random', out_path='OutData/Models/groundsplit_400gfr7/', rex_path='InData/RealGames.txt', split_path='OutData/Splits/ground-truth-split.pklz', wrdl_crit=['most_common', '400'])\"]\n"
     ]
    }
   ],
   "source": [
    "gengoogle = wacgen.WacGen(saia=False,google=True,groundsplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len 1\n",
      "Train len 2\n",
      "Train len 3\n",
      "Train len 4\n",
      "Train len 5\n"
     ]
    }
   ],
   "source": [
    "_,_,lengoogle = gengoogle.get_len_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "google_gen_list = []\n",
    "for (_,row) in alltest.iterrows():\n",
    "    #print (row['file'],row['region'])\n",
    "    try:\n",
    "        gen_utt2 = gengoogle.generate_beam_plen(lengoogle,row['file'],row['region'])\n",
    "    except:\n",
    "        gen_utt2 = ['','']\n",
    "    google_gen_list.append(' '.join(gen_utt2[1:]))\n",
    "    \n",
    "alltest['google'] = google_gen_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not generate (8103, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not generate (19579, 1)\n"
     ]
    }
   ],
   "source": [
    "google_glen_list = []\n",
    "for (_,row) in alltest.iterrows():\n",
    "    #print (row['file'],row['region'])\n",
    "    try:\n",
    "        gen_utt2 = gengoogle.generate_beam(row['file'],row['region'],len(row['Referring Expression'].split()))\n",
    "    except:\n",
    "        print \"Could not generate\", (row['file'],row['region'])\n",
    "        gen_utt2 = ['','']\n",
    "        \n",
    "    google_glen_list.append(' '.join(gen_utt2[1:]))\n",
    "    \n",
    "alltest['google_glen'] = google_glen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alltest.rename(columns={'Referring Expression':'gold'},inplace=True)\n",
    "allout = alltest[['Testset','file','region','gold','google','saia','google_glen']]\n",
    "allout.to_csv('EvalExp1/exp1_ground_truth_generated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *EvalExp1/exp1_ground_truth_generated.csv* is the output file that will be used by the evaluation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, you have to find some human subjects to evaluate the generated expressions.\n",
    "\n",
    "\n",
    "You have to sit them in front of the computer and let them play the ReferitGame using the GUI\n",
    "in EvalExp1.\n",
    "\n",
    "**cd EvalExp1**\n",
    "\n",
    "Just start the GUI as\n",
    "\n",
    "**python evalgui_exp1.py -n N**\n",
    "where N is the number of games the subject wants to play\n",
    "\n",
    "this will produce log files stored in **EvalExp1/final_logs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Installment-based REG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we generate referring expressions with a single wac (words-as-classifiers) model (based on googLE features), but using different modes of interactive delivery or presentation.\n",
    "\n",
    "Thus, in case the user clicks on the wrong object in the first trial, we want to be able to correct or elaborate the refexp.\n",
    "\n",
    "This is a more interactive experiment, but we will still generate all expressions for an object offline, according to two different strategies for installment-based REG.\n",
    "\n",
    "Thus, the output file will have a slightly different format (additional columns for installments). And the GUI that present the refexp is changed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* as in Experiment 1, refexp. are generated off-line\n",
    "* if you want, you can skip this step and use the file\n",
    "*EvalExp2/exp2_ground_truth_generated.csv*\n",
    "* the following steps will show how to produce this file for the given generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wacgen_ia' from 'wacgen_ia.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wacgen_ia\n",
    "reload(wacgen_ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word classifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wacgen_ia.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.testdf['file'] = files\n",
      "wacgen_ia.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.testdf['region'] = regions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded testfiles 1500\n",
      "Excluded classifiers all ,tv ,to ,sorry ,gravel ,sitting ,blanket ,wooden ,peeps ,biker ,hanging ,second ,click ,what ,section ,man's ,full ,leg ,path ,greenery ,standing ,from ,two ,next ,this ,nearest ,grassy ,court ,1 ,tile ,beige ,okay ,structure ,spot ,plane ,end ,arch ,branches ,third ,coat ,over ,looks ,backpack ,main ,them ,dat ,half ,rocky ,beneath ,glasses ,spider ,sandy ,out ,rt ,3rd ,space ,looking ,foreground ,dirt ,where ,woman's ,first ,one ,feet ,directly ,open ,anyone ,their ,2 ,legs ,that ,shelf ,part ,distance ,and ,infront ,any ,shorts ,- ,so ,towards ,object ,most ,clear ,walking ,bright ,wearing ,his ,striped ,patch ,stuff ,ruins ,farthest ,seat ,see ,are ,close ,arm ,away ,3 ,hammock ,last ,womans ,whole ,2nd ,blonde ,three ,beer ,tallest ,empty ,near ,is ,it ,nightstand ,thingy ,things ,bowl ,beside ,wheel ,closest ,hand ,off ,i ,edge ,just ,photo ,hands ,mid ,yep ,bit ,has ,around ,couple ,whatever ,shoulder ,like ,piece ,facing ,either ,old ,crowd ,back ,dead ,hair ,by ,anything ,of ,truck ,range ,or ,image ,ppl ,down ,her ,there ,lol ,low ,was ,only ,but ,walkway ,line ,with ,up ,us ,at ,no ,field ,holding ,you ,guy's ,pool ,cliff ,furthest ,phone ,portion\n",
      "A1 classifiers 19\n",
      "A2 classifiers 195\n",
      "A3 classifiers 216\n",
      "Type classifiers paper ,spiders ,railing ,chair ,birds ,row ,group ,hills ,tire ,cup ,sky ,lake ,bench ,bricks ,window ,bike ,pile ,board ,orange ,hat ,plate ,woman ,branch ,food ,trees ,llama ,desk ,shadow ,lady ,clouds ,jacket ,chairs ,pillar ,rocks ,bag ,sand ,steps ,rock ,chick ,sidewalk ,river ,side ,square ,fence ,people ,house ,fish ,strip ,plant ,sign ,street ,palm ,roof ,sea ,seal ,girl ,cactus ,flower ,leaf ,shrub ,clock ,sun ,pavement ,ice ,lights ,fountain ,hill ,ground ,bird ,pants ,umbrella ,screen ,men ,bldg ,water ,houses ,box ,trunk ,van ,post ,guy ,bushes ,boy ,stone ,kids ,column ,island ,leaves ,doorway ,fruit ,stand ,smoke ,road ,mans ,wall ,girls ,pot ,dudes ,pole ,church ,table ,cloud ,city ,horse ,heads ,area ,sweater ,dude ,bush ,camera ,way ,vehicle ,curtain ,brick ,kid ,frame ,curtains ,ceiling ,door ,bus ,hut ,glass ,flag ,train ,child ,flowers ,stairs ,women ,plants ,wood ,animals ,car ,cap ,tree ,bed ,books ,mtn ,can ,cake ,grass ,bridge ,mountains ,pic ,head ,shrubs ,ocean ,balcony ,mountain ,shirt ,floor ,snow ,lamp ,book ,animal ,gate ,beach ,picture ,buildings ,tower ,persons ,waterfall ,statue ,waves ,guys ,man ,building ,land ,windows ,light ,dog ,face ,beds ,person ,boat ,bottle ,painting\n"
     ]
    }
   ],
   "source": [
    "genia = wacgen_ia.WacGenIA()\n",
    "genia.make_ia_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate the expressions\n",
    "(this will take a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not generate 8103 1\n",
      "Could not generate 8103 2\n",
      "Could not generate 8103 3\n",
      "Could not generate 19579 1\n",
      "Could not generate 19579 2\n",
      "No Features for 8103 1\n",
      "No Features for 8103 2\n",
      "No Features for 8103 3\n",
      "No Features for 19579 1\n",
      "No Features for 19579 2\n",
      "No distinguishing att expr found! 4872 8\n",
      "No distinguishing att expr found! 16523 7\n",
      "No distinguishing att expr found! 1735 12\n",
      "No distinguishing att expr found! 5013 11\n",
      "No distinguishing att expr found! 7401 3\n",
      "No distinguishing att expr found! 8006 11\n"
     ]
    }
   ],
   "source": [
    "genia.generate_ia_distractors()\n",
    "genia.generate_type_distractors()\n",
    "att_l = genia.generate_ia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genia.testdf['ref_a1'] = [\" \".join(u[0][1:]) for u in att_l]\n",
    "genia.testdf['ref_a2'] = [\" \".join(u[1][1:]) for u in att_l]\n",
    "genia.testdf['ref_a3'] = [\" \".join(u[2][1:]) for u in att_l]\n",
    "genia.testdf['ref_h_a2'] = [\" \".join(u[3][1:]) for u in att_l]\n",
    "genia.testdf['ref_h_a3'] = [\" \".join(u[4][1:]) for u in att_l]\n",
    "genia.testdf['ref_h_nouns'] = [u[5] for u in att_l]\n",
    "genia.testdf['ref_gentype'] = [u[6] for u in att_l]\n",
    "\n",
    "gfile = './EvalExp2/exp2_ground_truth_generated.csv'\n",
    "genia.testdf.to_csv(gfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have to find some human subjects to evaluate the generated expressions.\n",
    "\n",
    "\n",
    "You have to sit them in front of the computer and let them play the ReferitGame using the GUI\n",
    "in EvalExp2.\n",
    "\n",
    "**cd EvalExp2**\n",
    "\n",
    "Just start the GUI as\n",
    "\n",
    "* **python evalgui_ia.py -n N** (where N is the number of games the subject wants to play)\n",
    "\n",
    "* **python evalgui_ia_bl.py -n N** (where N is the number of games the subject wants to play)\n",
    "\n",
    "this will produce log files stored in **EvalExp2/final_logs**\n",
    "\n",
    "For this evaluation, you need 2 groups of subjects:\n",
    " * g1: playing evalgui_ia\n",
    " * g2: playing evalgui_ia_bl\n",
    " \n",
    "\n",
    "This is different in Experiment 1, where each subject sees refexp generated by different systems\n",
    "(but the same overall non-interactive strategy).\n",
    " \n",
    "Thus, each subject only sees 1 version of the interactive REG,\n",
    "the baseline (evalgui_ia_bl) or the context-aware installments version (evalgui_ia).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
